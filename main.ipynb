{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1c87203b",
      "metadata": {
        "id": "1c87203b"
      },
      "source": [
        "# ProjectWork Deep Learning\n",
        "https://www.kaggle.com/competitions/mlnomads-mlolympiad24/overview"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation_models_pytorch\n",
        "!pip install albumentations"
      ],
      "metadata": {
        "id": "uwTvtWGEkkwJ",
        "outputId": "565b477a-8693-4777-fdff-9eef954dec7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "uwTvtWGEkkwJ",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation_models_pytorch\n",
            "  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (11.1.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (0.5.3)\n",
            "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (1.0.15)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8->segmentation_models_pytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8->segmentation_models_pytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8->segmentation_models_pytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8->segmentation_models_pytorch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8->segmentation_models_pytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8->segmentation_models_pytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8->segmentation_models_pytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8->segmentation_models_pytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8->segmentation_models_pytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8->segmentation_models_pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation_models_pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation_models_pytorch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2025.1.31)\n",
            "Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, segmentation_models_pytorch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 segmentation_models_pytorch-0.5.0\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.14.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.3)\n",
            "Requirement already satisfied: albucore==0.0.23 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.23)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (3.12.3)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (6.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fc9b3b64",
      "metadata": {
        "id": "fc9b3b64"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import segmentation_models_pytorch as smp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b8164fa2",
      "metadata": {
        "id": "b8164fa2",
        "outputId": "291060d7-9054-473c-a341-34a52c6da034",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae241d69",
      "metadata": {
        "id": "ae241d69"
      },
      "source": [
        "## Upload Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a4f5da2f",
      "metadata": {
        "id": "a4f5da2f",
        "outputId": "6ff2fe1a-8c7c-40a7-9596-f38c4460640e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/mlolympiads24/train/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Paths\n",
        "\n",
        "# TRAIN_IMG_DIR = \"./Data/train/\"\n",
        "# TRAIN_MASK_DIR = \"./Data/train_labels/\"\n",
        "# TEST_IMG_DIR = \"./test/\"\n",
        "\n",
        "TRAIN_IMG_DIR = \"/content/drive/MyDrive/mlolympiads24/train/\"\n",
        "TRAIN_MASK_DIR = \"/content/drive/MyDrive/mlolympiads24/train_labels/\"\n",
        "TEST_IMG_DIR = \"/content/drive/MyDrive/mlolympiads24/test/\"\n",
        "\n",
        "IMG_SIZE = 256\n",
        "\n",
        "TRAIN_IMG_DIR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "66a8ef54",
      "metadata": {
        "id": "66a8ef54",
        "outputId": "c03f920f-b786-49b1-a3e6-823a3600f47f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.11.0) /io/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-eec4be213da7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mvisualize_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"img_001\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-eec4be213da7>\u001b[0m in \u001b[0;36mvisualize_sample\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvisualize_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_IMG_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{idx}.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_MASK_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{idx}.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) /io/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ],
      "source": [
        "# Visualize sample\n",
        "def visualize_sample(idx):\n",
        "    img = cv2.imread(os.path.join(TRAIN_IMG_DIR, f\"{idx}.jpg\"))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    mask = cv2.imread(os.path.join(TRAIN_MASK_DIR, f\"{idx}.png\"), 0)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(\"Image\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(mask, cmap=\"gray\")\n",
        "    plt.title(\"Water Mask\")\n",
        "    plt.show()\n",
        "\n",
        "visualize_sample(\"img_001\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4da17fce",
      "metadata": {
        "id": "4da17fce"
      },
      "source": [
        "## Explore Data\n",
        "- Water vs non-water distribution\n",
        "- Image diversity: snow, desert, coast, etc.\n",
        "- Empty masks (no water)\n",
        "- Potential class imbalance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea79fa85",
      "metadata": {
        "id": "ea79fa85"
      },
      "source": [
        "## Build a Baseline Model\n",
        "Start with a simple U-Net or DeepLabV3+ model:\n",
        "- Use pre-trained backbones (like resnet34, efficientnet) via libraries like segmentation_models.pytorch or tensorflow.keras.applications.\n",
        "- Input: 3-channel RGB\n",
        "- Output: 256x256 binary mask (sigmoid output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb802dd9",
      "metadata": {
        "id": "fb802dd9"
      },
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "class WaterDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir=None, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.image_ids = sorted(os.listdir(image_dir))\n",
        "        self.transform = transform\n",
        "        self.has_mask = mask_dir is not None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.image_ids[idx]\n",
        "        img = cv2.imread(os.path.join(self.image_dir, img_id))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.has_mask:\n",
        "            mask_path = os.path.join(self.mask_dir, img_id.replace(\".jpg\", \".png\"))\n",
        "            mask = cv2.imread(mask_path, 0)\n",
        "            mask = np.expand_dims(mask, axis=-1)\n",
        "        else:\n",
        "            mask = np.zeros((IMG_SIZE, IMG_SIZE, 1), dtype=np.uint8)\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=img, mask=mask)\n",
        "            img = augmented[\"image\"]\n",
        "            mask = augmented[\"mask\"]\n",
        "        return img, mask.float()\n",
        "\n",
        "# Transforms\n",
        "train_transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.Normalize(),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Normalize(),\n",
        "    ToTensorV2(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46289a37",
      "metadata": {
        "id": "46289a37"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "model = smp.Unet(\n",
        "    encoder_name=\"resnet34\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "999a89f5",
      "metadata": {
        "id": "999a89f5"
      },
      "source": [
        "## Augmentations & Dataloaders\n",
        "Use Albumentations for realistic augmentations and Create a custom Dataset class for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc3d714b",
      "metadata": {
        "id": "bc3d714b"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "transform = A.Compose([\n",
        "    A.HorizontalFlip(),\n",
        "    A.VerticalFlip(),\n",
        "    A.RandomBrightnessContrast(),\n",
        "    A.Normalize(),\n",
        "    ToTensorV2()\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59bfa704",
      "metadata": {
        "id": "59bfa704"
      },
      "source": [
        "## Loss Functions\n",
        "Dice loss is your best friend here. Combine it with BCE for stability. Or use smp.losses.DiceLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "601c175c",
      "metadata": {
        "id": "601c175c"
      },
      "outputs": [],
      "source": [
        "# Loss & Optimizer\n",
        "def dice_loss(pred, target, smooth=1.):\n",
        "    pred = pred.contiguous()\n",
        "    target = target.contiguous()\n",
        "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
        "    loss = 1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth))\n",
        "    return loss.mean()\n",
        "\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def loss_fn(pred, target):\n",
        "    return 0.5 * bce(pred, target) + 0.5 * dice_loss(torch.sigmoid(pred), target)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fb4b397",
      "metadata": {
        "id": "8fb4b397"
      },
      "source": [
        "## Training & Validation Loop\n",
        "Train your model on the dataset, validate on a hold-out split or cross-validation fold.\n",
        "\n",
        "Track:\n",
        "- Dice score on val set\n",
        "- Loss curves\n",
        "- Qualitative predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb8f742c",
      "metadata": {
        "id": "bb8f742c"
      },
      "outputs": [],
      "source": [
        "# Train Loop\n",
        "def train_fn(model, loader, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for imgs, masks in tqdm(loader):\n",
        "        imgs, masks = imgs.cuda(), masks.cuda()\n",
        "        preds = model(imgs)\n",
        "        loss = loss_fn(preds, masks)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "# Validation Loop\n",
        "def validate_fn(model, loader):\n",
        "    model.eval()\n",
        "    dices = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, masks in loader:\n",
        "            imgs, masks = imgs.cuda(), masks.cuda()\n",
        "            preds = torch.sigmoid(model(imgs))\n",
        "            preds = (preds > 0.5).float()\n",
        "            intersection = (preds * masks).sum()\n",
        "            union = preds.sum() + masks.sum()\n",
        "            dice = (2. * intersection) / (union + 1e-7)\n",
        "            dices.append(dice.item())\n",
        "    return np.mean(dices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72bc65f8",
      "metadata": {
        "id": "72bc65f8"
      },
      "outputs": [],
      "source": [
        "# Train & Eval\n",
        "model = model.cuda()\n",
        "\n",
        "train_ds = WaterDataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, transform=train_transform)\n",
        "val_ds = WaterDataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, transform=val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=8)\n",
        "\n",
        "for epoch in range(10):\n",
        "    train_loss = train_fn(model, train_loader, loss_fn, optimizer)\n",
        "    val_score = validate_fn(model, val_loader)\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Dice: {val_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3971f226",
      "metadata": {
        "id": "3971f226"
      },
      "source": [
        "## Prediction & RLE Encoding\n",
        "After prediction, convert binary masks into RLE format for submission:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46f2d62d",
      "metadata": {
        "id": "46f2d62d"
      },
      "outputs": [],
      "source": [
        "# Prediction & Submission\n",
        "def mask_to_rle(mask):\n",
        "    pixels = mask.flatten(order=\"F\")\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    return \" \".join(str(x) for x in runs)\n",
        "\n",
        "test_ds = WaterDataset(TEST_IMG_DIR, transform=val_transform)\n",
        "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "rles = []\n",
        "image_names = sorted(os.listdir(TEST_IMG_DIR))\n",
        "\n",
        "for i, (img, _) in enumerate(tqdm(test_loader)):\n",
        "    img = img.cuda()\n",
        "    with torch.no_grad():\n",
        "        pred = torch.sigmoid(model(img))[0, 0].cpu().numpy()\n",
        "        mask = (pred > 0.5).astype(np.uint8)\n",
        "        rle = mask_to_rle(mask)\n",
        "        rles.append([image_names[i], rle])\n",
        "\n",
        "submission = pd.DataFrame(rles, columns=[\"image_name\", \"rle_mask\"])\n",
        "submission.to_csv(\"submission.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f54e36a6",
      "metadata": {
        "id": "f54e36a6"
      },
      "source": [
        "## Optimize & Experiment\n",
        "Try better encoders (EfficientNet, Swin Transformer, ConvNeXt)\n",
        "- Use TTA (test-time augmentation)\n",
        "- Try attention-based U-Nets\n",
        "- Use pseudo-labeling or ensemble different models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f37520b3",
      "metadata": {
        "id": "f37520b3"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}