{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c87203b",
   "metadata": {},
   "source": [
    "# ProjectWork Deep Learning\n",
    "https://www.kaggle.com/competitions/mlnomads-mlolympiad24/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b3b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“¦ 1. Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import segmentation_models_pytorch as smp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8164fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae241d69",
   "metadata": {},
   "source": [
    "## Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f5da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "\n",
    "# TRAIN_IMG_DIR = \"./Data/train/\"\n",
    "# TRAIN_MASK_DIR = \"./Data/train_labels/\"\n",
    "# TEST_IMG_DIR = \"./test/\"\n",
    "\n",
    "TRAIN_IMG_DIR = \"/content/drive/MyDrive/MoroccoDataset/train/\"\n",
    "TRAIN_MASK_DIR = \"/content/drive/MyDrive/MoroccoDataset/train_labels/\"\n",
    "TEST_IMG_DIR = \"/content/drive/MyDrive/MoroccoDataset/test/\"\n",
    "\n",
    "IMG_SIZE = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a8ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample\n",
    "def visualize_sample(idx):\n",
    "    img = cv2.imread(os.path.join(TRAIN_IMG_DIR, f\"{idx}.jpg\"))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    mask = cv2.imread(os.path.join(TRAIN_MASK_DIR, f\"{idx}.png\"), 0)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Image\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(mask, cmap=\"gray\")\n",
    "    plt.title(\"Water Mask\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_sample(\"img_001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da17fce",
   "metadata": {},
   "source": [
    "## Explore Data\n",
    "- Water vs non-water distribution\n",
    "- Image diversity: snow, desert, coast, etc.\n",
    "- Empty masks (no water)\n",
    "- Potential class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea79fa85",
   "metadata": {},
   "source": [
    "## Build a Baseline Model\n",
    "Start with a simple U-Net or DeepLabV3+ model:\n",
    "- Use pre-trained backbones (like resnet34, efficientnet) via libraries like segmentation_models.pytorch or tensorflow.keras.applications.\n",
    "- Input: 3-channel RGB\n",
    "- Output: 256x256 binary mask (sigmoid output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb802dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class WaterDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir=None, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_ids = sorted(os.listdir(image_dir))\n",
    "        self.transform = transform\n",
    "        self.has_mask = mask_dir is not None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img = cv2.imread(os.path.join(self.image_dir, img_id))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.has_mask:\n",
    "            mask_path = os.path.join(self.mask_dir, img_id.replace(\".jpg\", \".png\"))\n",
    "            mask = cv2.imread(mask_path, 0)\n",
    "            mask = np.expand_dims(mask, axis=-1)\n",
    "        else:\n",
    "            mask = np.zeros((IMG_SIZE, IMG_SIZE, 1), dtype=np.uint8)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img, mask=mask)\n",
    "            img = augmented[\"image\"]\n",
    "            mask = augmented[\"mask\"]\n",
    "        return img, mask.float()\n",
    "    \n",
    "# Transforms\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46289a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999a89f5",
   "metadata": {},
   "source": [
    "## Augmentations & Dataloaders\n",
    "Use Albumentations for realistic augmentations and Create a custom Dataset class for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(),\n",
    "    A.VerticalFlip(),\n",
    "    A.RandomBrightnessContrast(),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bfa704",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "Dice loss is your best friend here. Combine it with BCE for stability. Or use smp.losses.DiceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss & Optimizer\n",
    "def dice_loss(pred, target, smooth=1.):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    loss = 1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth))\n",
    "    return loss.mean()\n",
    "\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def loss_fn(pred, target):\n",
    "    return 0.5 * bce(pred, target) + 0.5 * dice_loss(torch.sigmoid(pred), target)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb4b397",
   "metadata": {},
   "source": [
    "## Training & Validation Loop\n",
    "Train your model on the dataset, validate on a hold-out split or cross-validation fold.\n",
    "\n",
    "Track:\n",
    "- Dice score on val set\n",
    "- Loss curves\n",
    "- Qualitative predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Loop\n",
    "def train_fn(model, loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for imgs, masks in tqdm(loader):\n",
    "        imgs, masks = imgs.cuda(), masks.cuda()\n",
    "        preds = model(imgs)\n",
    "        loss = loss_fn(preds, masks)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(loader)\n",
    "\n",
    "# Validation Loop\n",
    "def validate_fn(model, loader):\n",
    "    model.eval()\n",
    "    dices = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in loader:\n",
    "            imgs, masks = imgs.cuda(), masks.cuda()\n",
    "            preds = torch.sigmoid(model(imgs))\n",
    "            preds = (preds > 0.5).float()\n",
    "            intersection = (preds * masks).sum()\n",
    "            union = preds.sum() + masks.sum()\n",
    "            dice = (2. * intersection) / (union + 1e-7)\n",
    "            dices.append(dice.item())\n",
    "    return np.mean(dices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc65f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Eval\n",
    "model = model.cuda()\n",
    "\n",
    "train_ds = WaterDataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, transform=train_transform)\n",
    "val_ds = WaterDataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=8)\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss = train_fn(model, train_loader, loss_fn, optimizer)\n",
    "    val_score = validate_fn(model, val_loader)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Dice: {val_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3971f226",
   "metadata": {},
   "source": [
    "## Prediction & RLE Encoding\n",
    "After prediction, convert binary masks into RLE format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f2d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction & Submission\n",
    "def mask_to_rle(mask):\n",
    "    pixels = mask.flatten(order=\"F\")\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return \" \".join(str(x) for x in runs)\n",
    "\n",
    "test_ds = WaterDataset(TEST_IMG_DIR, transform=val_transform)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "rles = []\n",
    "image_names = sorted(os.listdir(TEST_IMG_DIR))\n",
    "\n",
    "for i, (img, _) in enumerate(tqdm(test_loader)):\n",
    "    img = img.cuda()\n",
    "    with torch.no_grad():\n",
    "        pred = torch.sigmoid(model(img))[0, 0].cpu().numpy()\n",
    "        mask = (pred > 0.5).astype(np.uint8)\n",
    "        rle = mask_to_rle(mask)\n",
    "        rles.append([image_names[i], rle])\n",
    "\n",
    "submission = pd.DataFrame(rles, columns=[\"image_name\", \"rle_mask\"])\n",
    "submission.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e36a6",
   "metadata": {},
   "source": [
    "## Optimize & Experiment\n",
    "Try better encoders (EfficientNet, Swin Transformer, ConvNeXt)\n",
    "- Use TTA (test-time augmentation)\n",
    "- Try attention-based U-Nets\n",
    "- Use pseudo-labeling or ensemble different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37520b3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
